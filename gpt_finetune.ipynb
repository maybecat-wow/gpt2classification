{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from zeus.monitor import ZeusMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naver_review_examples():\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "    test_data = pd.read_table('ratings_test.txt')\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "naver_data = get_naver_review_examples()\n",
    "\n",
    "dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SentimentClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(GPT2SentimentClassifier, self).__init__()\n",
    "\n",
    "        self.gpt_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n",
    "        self.drop = torch.nn.Dropout(p=0.1)\n",
    "        self.out = torch.nn.Linear(self.gpt_model.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.gpt_model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = hidden_states[0]\n",
    "        \n",
    "        output = self.drop(last_hidden_state[:, -1, :])\n",
    "\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_clf = GPT2SentimentClassifier(n_classes=1)\n",
    "gpt_clf.train()\n",
    "\n",
    "learning_rate = 5e-5\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(gpt_clf.parameters(), lr=learning_rate)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "epochs = 11\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_correct_num(predicts, labels):\n",
    "    predicts_ = predicts >= 0.5\n",
    "    correct_num = torch.sum(predicts_ == labels)\n",
    "        \n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loader(dataset,max_batch_size,shuffle=True):\n",
    "    if batch_size>max_batch_size:\n",
    "        maximized=True\n",
    "        return DataLoader(dataset,batch_size=batch_size-1,shuffle=True)\n",
    "    return DataLoader(dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoGPT-2 Training Start!\n",
      "[2023-12-12 20:51:56,387] [zeus.monitor.energy](energy.py:157) Monitoring GPU indices [0].\n",
      "current batch size: 4\n",
      "Epoch 0 consumed 1056.1363623142242 s and 86853.98999999999 J.\n",
      "One step took 0.10301281726360322 s and 8.496719100000005 J on average.\n",
      "current batch size: 5\n",
      "Epoch 1 consumed 921.3956432342529 s and 76666.826 J.\n",
      "One step took 0.11245977991819382 s and 9.381312374999958 J on average.\n",
      "current batch size: 6\n",
      "Epoch 2 consumed 866.1784181594849 s and 70673.657 J.\n",
      "One step took 0.12727255339884982 s and 10.414939253037264 J on average.\n",
      "current batch size: 7\n",
      "Epoch 3 consumed 843.7583587169647 s and 68242.13100000005 J.\n",
      "One step took 0.14472319197466993 s and 11.69603797025359 J on average.\n",
      "current batch size: 8\n",
      "Epoch 4 consumed 760.541571855545 s and 65775.38 J.\n",
      "One step took 0.14906626534461975 s and 12.930494799999963 J on average.\n",
      "current batch size: 9\n",
      "Epoch 5 consumed 721.1694252490997 s and 63994.64199999999 J.\n",
      "One step took 0.15941453639782455 s and 14.184499662541926 J on average.\n",
      "current batch size: 10\n",
      "Epoch 6 consumed 691.6246032714844 s and 59932.122999999905 J.\n",
      "One step took 0.16970400297641755 s and 14.738370250000123 J on average.\n",
      "current batch size: 11\n",
      "Epoch 7 consumed 669.4493677616119 s and 60336.341000000015 J.\n",
      "One step took 0.1808892763477123 s and 16.332642012647856 J on average.\n",
      "current batch size: 12\n",
      "Epoch 8 consumed 693.6646897792816 s and 61806.484999999986 J.\n",
      "One step took 0.20465687705716762 s and 18.271314937012498 J on average.\n",
      "current batch size: 13\n",
      "Epoch 9 consumed 1282.1016719341278 s and 99994.70499999996 J.\n",
      "One step took 0.4131334814345019 s and 32.29988462788442 J on average.\n",
      "current batch size: 14\n",
      "Epoch 10 consumed 2872.2900228500366 s and 189423.40800000005 J.\n",
      "One step took 1.0014093435252773 s and 66.0807963610917 J on average.\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "tot_train_loss = 0.0\n",
    "tot_valid_loss = 0.0\n",
    "\n",
    "train_correct_num = 0\n",
    "valid_correct_num = 0\n",
    "\n",
    "prev_valid_loss = 10000\n",
    "traindone=False\n",
    "print('KoGPT-2 Training Start!')\n",
    "monitor = ZeusMonitor(gpu_indices=[torch.cuda.current_device()])\n",
    "#plo = GlobalPowerLimitOptimizer(monitor)\n",
    "batch_size=4\n",
    "batch_effect={}\n",
    "maximized=False\n",
    "for epoch in range(epochs):\n",
    "    monitor.begin_window(\"epoch\")\n",
    "    #plo.on_epoch_begin()\n",
    "    measurements = []\n",
    "    print(f'current batch size: {batch_size}')\n",
    "    train_dataloader = custom_loader(train_set, max_batch_size=14,shuffle=True)\n",
    "    valid_dataloader = custom_loader(valid_set, max_batch_size=14,shuffle=True)\n",
    "    test_dataloader = custom_loader(test_set, max_batch_size=14,shuffle=True)\n",
    "    for batch, train_data in enumerate(train_dataloader):\n",
    "        monitor.begin_window(\"step\")\n",
    "        #plo.on_step_begin()\n",
    "        gpt_clf.to(device)\n",
    "        train_inputs = train_data['input_ids'].to(device)\n",
    "        train_masks = train_data['attention_mask'].to(device)\n",
    "        train_labels = train_data['labels'].to(device)\n",
    "        \n",
    "        train_outputs = gpt_clf(train_inputs, train_masks)\n",
    "        \n",
    "        train_loss = criterion(train_outputs.view(-1), train_labels.float())\n",
    "        \n",
    "            \n",
    "        valid_data = next(iter(valid_dataloader))\n",
    "\n",
    "        gpt_clf.to(device)\n",
    "        valid_inputs = valid_data['input_ids'].to(device)    \n",
    "        valid_masks = valid_data['attention_mask'].to(device)\n",
    "        valid_labels = valid_data['labels'].to(device)\n",
    "        \n",
    "        valid_outputs = gpt_clf(valid_inputs, valid_masks)\n",
    "        \n",
    "        valid_loss = criterion(valid_outputs.view(-1), valid_labels.float())\n",
    "        \n",
    "        gpt_clf.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tot_train_loss += train_loss.item()\n",
    "        tot_valid_loss += valid_loss.item()\n",
    "        \n",
    "        train_correct_num += cal_correct_num(torch.sigmoid(train_outputs.view(-1)), train_labels.float())\n",
    "        valid_correct_num += cal_correct_num(torch.sigmoid(valid_outputs.view(-1)), valid_labels.float())\n",
    "        result = monitor.end_window(\"step\")\n",
    "        #plo.on_step_end()\n",
    "        measurements.append(result)\n",
    "        if count % 200 == 0:\n",
    "            cnt = ((count+1) * batch_size)\n",
    "            current_train_loss = tot_train_loss / cnt\n",
    "            current_valid_loss = tot_valid_loss / cnt\n",
    "            \n",
    "            train_acc = train_correct_num / cnt\n",
    "            valid_acc = valid_correct_num / cnt\n",
    "            \n",
    "            #print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f | train_acc : %.5f | valid_acc : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss, train_acc, valid_acc))\n",
    "            \n",
    "            tot_train_loss = 0.0\n",
    "            tot_valid_loss = 0.0\n",
    "            \n",
    "            train_correct_num = 0\n",
    "            valid_correct_num = 0\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            if prev_valid_loss > current_valid_loss:\n",
    "                prev_valid_loss = current_valid_loss\n",
    "                torch.save(gpt_clf.state_dict(), f'./KoGPT-Classifier-model.pth')\n",
    "            if valid_acc>0.9:\n",
    "                traindone=True\n",
    "                break\n",
    "        count += 1\n",
    "    eres = monitor.end_window(\"epoch\")\n",
    "    print(f\"Epoch {epoch} consumed {eres.time} s and {eres.total_energy} J.\")\n",
    "    #plo.on_epoch_end()\n",
    "\n",
    "    avg_time = sum(map(lambda m: m.time, measurements)) / len(measurements)\n",
    "    avg_energy = sum(map(lambda m: m.total_energy, measurements)) / len(measurements)\n",
    "    batch_effect[batch_size]=eres.total_energy\n",
    "    print(f\"One step took {avg_time} s and {avg_energy} J on average.\")\n",
    "    if traindone==True:\n",
    "        break\n",
    "    if maximized ==False:\n",
    "        batch_size+=1\n",
    "    else:\n",
    "        batch_size=min(batch_effect,key=batch_effect.get)\n",
    "print(f'optimal batch size: {batch_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
